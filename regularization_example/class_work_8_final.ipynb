{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook For DS4400 Lecture 8\n",
    "\n",
    "By: John Henry Rudden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso as SKLasso, Ridge as SKRidge\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from enum import Enum\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('./data/stock_prediction_data.csv', delimiter=',')\n",
    "y = np.genfromtxt('./data/stock_price.csv', delimiter=',')\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Train, Validation, and Test Sets\n",
    "\n",
    "Use train to train the model, validation to compare models, and test to evaluate the final selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling our Data\n",
    "\n",
    "We will use SKLearn's StandardScaler to scale our data. This will make our data have a mean of 0 and a standard deviation of 1. \n",
    "It is important to scale our validation and test sets using the scaler that was fit to the training data. Since scaling is part of our preprocessing, we need to be consistent with the scaling across all of our data (train, validation, test, and future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use standard scaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Feature Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_train)\n",
    "X_val = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_val)\n",
    "X_test = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "As we saw in class, the objective for Lasso Regression is to minimize the following:\n",
    "\n",
    "$$\\min_w \\frac{1}{n} \\sum_{i=1}^n (\\phi(x_i)^Tw - y_i)^2 + \\lambda ||w||_1$$\n",
    "\n",
    "Where $\\lambda$ is a hyperparameter that we can tune to control the amount of regularization (weighting in importance of L1 norm in objective). Notice if $\\lambda = 0$, then we have the same objective as Linear Regression.\n",
    "\n",
    "As you can see from the objective, Lasso Regression is simply Linear Regression with an additional term that penalizes the absolute value of the coefficients (L1 Norm).\n",
    "\n",
    "### Taking the Derivative of the Objective\n",
    "\n",
    "We have also seen the $\\frac{\\partial}{\\partial w} ||w||_1$ before in class. This is simply:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} ||w||_1 = sign(w)$$\n",
    "\n",
    "Moreover, the complete derivative of the objective is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} \\frac{1}{n} \\sum_{i=1}^n (\\phi(x_i)^Tw - y_i)^2 + \\lambda ||w||_1 = \\frac{1}{n} \\sum_{i=1}^n 2\\phi(x_i)(\\phi(x_i)^Tw - y_i) + \\lambda sign(w)$$\n",
    "\n",
    "In this code I will be using a vectorized implementation of the gradient of the objective. This is because it is much faster than using a for loop to calculate the gradient This vectorized gradient of the objective is as follows:\n",
    "\n",
    "$$\\frac{2}{n} \\Phi^T(\\Phi w - y) + \\lambda sign(w)$$\n",
    "\n",
    "# Read if you are confused!\n",
    " If any of the code is confusing, please check out lecture 7 notebook where I do the full derivation of the vectorized implementation of Linear Regression. This can directly be applied to Lasso Regression and Ridge Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ߜLasso(w: ndarray, Φ: ndarray, y: ndarray, λ: float) -> ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the Lasso loss function with respect to weights (w).\n",
    "\n",
    "    Parameters:\n",
    "    - w: ndarray, The weight vector.\n",
    "    - Φ: ndarray, The feature matrix after applying the basis function. (Phi)\n",
    "    - y: ndarray, The target values.\n",
    "    - λ: float, The regularization parameter. (lambda)\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: Gradient of the Lasso Objective with respect to w.\n",
    "    \"\"\"\n",
    "    n, m = Φ.shape\n",
    "    return 2/n * Φ.T.dot(Φ.dot(w) - y) + λ * np.sign(w) # if λ = 0, this is the gradient of the MSE loss function\n",
    "\n",
    "def gradient_descent_lasso(Φ: ndarray, y: ndarray, α: float = 0.01, num_iter: int = 10_000, λ: float = 1) -> ndarray:\n",
    "    \"\"\"\n",
    "    Performs gradient descent on Lasso Rregression Objective to find optimal w vector.\n",
    "\n",
    "    Notice: This function is simply Linear Regression code from Lecture 7, except I changed one line. Which line?\n",
    "\n",
    "    Parameters:\n",
    "    - Φ: ndarray, The feature matrix (Phi).\n",
    "    - y: ndarray, The target values.\n",
    "    - α: float, The learning rate.\n",
    "    - num_iter: int, The number of training iterations.\n",
    "    - λ: float, The regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: The optimized weights vector.\n",
    "    \"\"\"\n",
    "    n, m = Φ.shape\n",
    "    w = np.zeros((m, 1))\n",
    "    for _ in range(num_iter):\n",
    "        gradient = ߜLasso(w, Φ, y, λ=λ)  # Gradient of Lasso Objective with respect to w\n",
    "        \n",
    "        # Checking for convergence (See note Lecture 7 Notebook)\n",
    "        if np.all(np.abs(gradient) < 1e-5) or np.isnan(gradient).any():\n",
    "            break\n",
    "            \n",
    "        # Check for gradient explosion (See note Lecture 7 Notebook)\n",
    "        if np.isinf(gradient).any(): \n",
    "            raise ValueError(\"Gradient exploded\")\n",
    "\n",
    "        w -= α * gradient\n",
    "    return w\n",
    "\n",
    "def predict(Φ: ndarray, w: ndarray) -> ndarray:\n",
    "    \"\"\"\n",
    "    Predicts the target values using the linear model.\n",
    "\n",
    "    Parameters:\n",
    "    - Φ: ndarray, The feature matrix. (Phi)\n",
    "    - w: ndarray, The weights vector.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: Predicted values.\n",
    "    \"\"\"\n",
    "    return Φ.dot(w)\n",
    "\n",
    "def mse(y: ndarray, y_hat: ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error (MSE) between actual and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y: ndarray, Actual target values.\n",
    "    - y_hat: ndarray, Predicted values by the model.\n",
    "\n",
    "    Returns:\n",
    "    - float: The mean squared error between actual and predicted values.\n",
    "    \"\"\"\n",
    "    return np.mean((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using My Lasso Regression Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Lasso Regression Train MSE: 2.122974823830284\n",
      "My Lasso Regression Validation MSE: 2.3665208202191623\n"
     ]
    }
   ],
   "source": [
    "w_lasso_gd = gradient_descent_lasso(X_train, y_train, λ=1)\n",
    "pred_train = predict(X_train, w_lasso_gd)\n",
    "print(f\"My Lasso Regression Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = predict(X_val, w_lasso_gd)\n",
    "print(f\"My Lasso Regression Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn's Lasso Regression\n",
    "\n",
    "In Sklearns Lasso regression, alpha corresponds to the lambda value in the lecture notes. The higher the alpha, the more the coefficients are pushed towards zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearn Lasso Train MSE: 6.831045121140056\n",
      "SKLearn Lasso Validation MSE: 7.629002980184824\n"
     ]
    }
   ],
   "source": [
    "sk_poly_lasso = SKLasso(alpha=1)\n",
    "sk_poly_lasso.fit(X_train,y_train.flatten()) # y is 2D, but scikit-learn expects 1D\n",
    "pred_train = sk_poly_lasso.predict(X_train).reshape(-1,1)\n",
    "print(f\"SKLearn Lasso Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = sk_poly_lasso.predict(X_val).reshape(-1,1)\n",
    "print(f\"SKLearn Lasso Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems Like for λ = 1, my Lasso Regression is Better (on Validation set)...\n",
    "\n",
    "You may be curious why this is the case. I am too. Check out [SKLearn's Lasso Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) to see if you can figure out why. Hint: do they use a learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Validation to Select Best Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Lasso Regression Validation MSE: 0.09295127686258214 for lambda: 0\n",
      "My Lasso Regression Validation MSE: 0.09554581350427133 for lambda: 0.1\n",
      "My Lasso Regression Validation MSE: 2.3665208202191623 for lambda: 1\n",
      "My Lasso Regression Validation MSE: 46.705761270943 for lambda: 10\n",
      "Best lambda: 0 with MSE: 0.09295127686258214\n"
     ]
    }
   ],
   "source": [
    "valid_lambdas = [0, 0.1, 1, 10] # list of values to try for lambda\n",
    "best_lambda_lasso = None # store the best lambda value\n",
    "best_mse_lasso = float('inf') # store the best mse value\n",
    "for λ in valid_lambdas:\n",
    "    w_lasso_gd = gradient_descent_lasso(X_train, y_train, λ=λ) # train the model with Train data and λ\n",
    "    pred_val = predict(X_val, w_lasso_gd) # predict the validation data\n",
    "    mse_ = mse(y_val, pred_val)\n",
    "    print(f\"My Lasso Regression Validation MSE: {mse_} for lambda: {λ}\")\n",
    "    if mse_ < best_mse_lasso:\n",
    "        best_mse_lasso = mse_\n",
    "        best_lambda_lasso = λ\n",
    "print(f\"Best lambda: {best_lambda_lasso} with MSE: {best_mse_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression Time!\n",
    "\n",
    "Instead of using the L1 Norm, Ridge Regression uses the L2 Norm. The objective for Ridge Regression is to minimize the following:\n",
    "\n",
    "$$\\min_w \\frac{1}{n} \\sum_{i=1}^n (\\phi(x_i)^Tw - y_i)^2 + \\lambda ||w||_2^2$$\n",
    "\n",
    "Just like in Lasso, $\\lambda$ is a hyperparameter that we can tune to control the amount of regularization. Similar to Lasso, if $\\lambda = 0$, then we have the same objective as Linear Regression.\n",
    "\n",
    "### Taking the Derivative of the Objective\n",
    "\n",
    "Taking the derivative of the squared L2 Norm is super simple:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} ||w||_2^2 = 2w$$\n",
    "\n",
    "Moreover, the derivative of the objective is also super simple:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} \\frac{1}{n} \\sum_{i=1}^n (\\phi(x_i)^Tw - y_i)^2 + \\lambda ||w||_2^2 = \\frac{1}{n} \\sum_{i=1}^n 2\\phi(x_i)(\\phi(x_i)^Tw - y_i) + 2\\lambda w$$\n",
    "\n",
    "Once again I will be using the vectorized implementation of the gradient of the objective. This is as follows:\n",
    "\n",
    "$$\\frac{2}{n} \\Phi^T(\\Phi w - y) + 2\\lambda w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ߜRidge(w: ndarray, Φ: ndarray, y: ndarray, λ: float) -> ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the Ridge loss function with respect to weights (w).\n",
    "\n",
    "    Parameters:\n",
    "    - w: ndarray, The weight vector.\n",
    "    - Φ: ndarray, The feature matrix after applying the basis function. (Phi)\n",
    "    - y: ndarray, The target values.\n",
    "    - λ: float, The regularization parameter. (lambda)\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: Gradient of the Ridge Objective with respect to w.\n",
    "    \"\"\"\n",
    "    n, m = Φ.shape\n",
    "    return 2/n * Φ.T.dot(Φ.dot(w) - y) + 2 * λ * w # if λ = 0, this is the gradient of the MSE loss function\n",
    "\n",
    "def gradient_descent_ridge(Φ: ndarray, y: ndarray, α: float = 0.01, num_iter: int = 10_000, λ: float = 1) -> ndarray:\n",
    "    \"\"\"\n",
    "    Performs gradient descent on Ridge Rregression Objective to find optimal w vector.\n",
    "\n",
    "    Parameters:\n",
    "    - Φ: ndarray, The feature matrix (Phi).\n",
    "    - y: ndarray, The target values.\n",
    "    - α: float, The learning rate.\n",
    "    - num_iter: int, The number of training iterations.\n",
    "    - λ: float, The regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: The optimized weights vector.\n",
    "    \"\"\"\n",
    "    n, m = Φ.shape\n",
    "    w = np.zeros((m, 1))\n",
    "    for _ in range(num_iter):\n",
    "        gradient = ߜRidge(w, Φ, y, λ=λ)  # Gradient of Ridge Objective with respect to w (Notice this is the only change from Lasso)\n",
    "        \n",
    "        # Checking for convergence (See note Lecture 7 Notebook)\n",
    "        if np.all(np.abs(gradient) < 1e-5) or np.isnan(gradient).any():\n",
    "            break\n",
    "            \n",
    "        # Check for gradient explosion (See note Lecture 7 Notebook)\n",
    "        if np.isinf(gradient).any(): \n",
    "            raise ValueError(\"Gradient exploded\")\n",
    "\n",
    "        w -= α * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Ridge Regression Train MSE: 12.639576707863057\n",
      "My Ridge Regression Validation MSE: 16.01545282823887\n"
     ]
    }
   ],
   "source": [
    "w_ridge_gd = gradient_descent_ridge(X_train, y_train, λ=1)\n",
    "pred_train = predict(X_train, w_ridge_gd)\n",
    "print(f\"My Ridge Regression Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = predict(X_val, w_ridge_gd)\n",
    "print(f\"My Ridge Regression Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SKLearn's Ridge Regression\n",
    "\n",
    "Once again, SKLearn's Ridge regression uses alpha instead of lambda. The higher the alpha, the more the coefficients are pushed towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearn Ridge Train MSE: 0.03284528040883205\n",
      "SKLearn Ridge Validation MSE: 0.10389999417330495\n"
     ]
    }
   ],
   "source": [
    "sk_poly_ridge = SKRidge(alpha=1)\n",
    "sk_poly_ridge.fit(X_train,y_train.flatten()) # y is 2D, but scikit-learn expects 1D\n",
    "pred_train = sk_poly_ridge.predict(X_train).reshape(-1,1)\n",
    "print(f\"SKLearn Ridge Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = sk_poly_ridge.predict(X_val).reshape(-1,1)\n",
    "print(f\"SKLearn Ridge Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn's Ridge Regression Seems to Perform Way Better Than My Ridge Regression...\n",
    "\n",
    "Once again, you may be curious why this is the case. Just like in the Lasso Regression case, SKLearn implements its learning a bit differently then we learned in class. Check out [SKLearn's Ridge Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) to see how. Hint: do they use a learning rate and what is the `solver` parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Best Lambda for Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Ridge Regression Validation MSE: 0.09295127686258214 for lambda: 0\n",
      "My Ridge Regression Validation MSE: 1.1402329224528371 for lambda: 0.1\n",
      "My Ridge Regression Validation MSE: 16.01545282823887 for lambda: 1\n",
      "My Ridge Regression Validation MSE: 42.66613532773383 for lambda: 10\n",
      "Best lambda: 0 with MSE: 0.09295127686258214\n"
     ]
    }
   ],
   "source": [
    "valid_lambdas = [0, 0.1, 1, 10] # condidate values for lambda (this is same process I used for Lasso)\n",
    "best_lambda_ridge = None # store the best lambda value\n",
    "best_mse_ridge = float('inf') # store the best mse value\n",
    "for λ in valid_lambdas:\n",
    "    w_ridge_gd = gradient_descent_ridge(X_train, y_train, λ=λ) \n",
    "    pred_val = predict(X_val, w_ridge_gd) # predict the validation data\n",
    "    mse_ = mse(y_val, pred_val)\n",
    "    print(f\"My Ridge Regression Validation MSE: {mse_} for lambda: {λ}\")\n",
    "    if mse_ < best_mse_ridge:\n",
    "        best_mse_ridge = mse_\n",
    "        best_lambda_ridge = λ\n",
    "print(f\"Best lambda: {best_lambda_ridge} with MSE: {best_mse_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Lasso and Ridge Regression\n",
    "\n",
    "Which one is better? Well we can compare the MSE of the validation set for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which model is better for our data?\n",
      "There seems to be no clear winner. Both models perform equally well.\n",
      "Linear Regression is recommended as the simpler model because lambda is 0.\n",
      "Test MSE: 0.0677020386538741\n"
     ]
    }
   ],
   "source": [
    "print('Which model is better for our data?')\n",
    "\n",
    "# Initial assumption that no model is selected\n",
    "best_lambda = None\n",
    "best_w = None\n",
    "\n",
    "if best_mse_lasso < best_mse_ridge:\n",
    "    print(\"Lasso is better\")\n",
    "    best_lambda = best_lambda_lasso\n",
    "    best_w = gradient_descent_lasso(X_train, y_train, λ=best_lambda)\n",
    "elif best_mse_ridge < best_mse_lasso:\n",
    "    print(\"Ridge is better\")\n",
    "    best_lambda = best_lambda_ridge\n",
    "    best_w = gradient_descent_ridge(X_train, y_train, λ=best_lambda)\n",
    "else:\n",
    "    print(\"There seems to be no clear winner. Both models perform equally well.\")\n",
    "    # If MSEs are equivalent, check if one of the lambda values is 0\n",
    "    if best_lambda_lasso == 0 or best_lambda_ridge == 0:\n",
    "        print(\"Linear Regression is recommended as the simpler model because lambda is 0.\")\n",
    "        # Assuming you have a function to perform Linear Regression\n",
    "        best_w = gradient_descent_lasso(X_train, y_train, λ=0) # since λ=0, this is just linear regression\n",
    "    else:\n",
    "        # If both lambdas are not 0 but are equal, choose one based on preference or additional criteria\n",
    "        print(\"Both models perform equally well. Choose based on additional criteria or preference.\")\n",
    "        best_lambda = best_lambda_lasso  # This is arbitrary; you could choose Ridge similarly\n",
    "        best_w = gradient_descent_lasso(X_train, y_train, λ=best_lambda)\n",
    "\n",
    "pred_test = predict(X_test, best_w)\n",
    "print(f\"Test MSE: {mse(y_test, pred_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
