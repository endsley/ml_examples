{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook For DS4400 Lecture 7\n",
    "\n",
    "By: John Henry Rudden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression as SKLinearRegression\n",
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('./data/stock_prediction_data.csv', delimiter=',')\n",
    "y = np.genfromtxt('./data/stock_price.csv', delimiter=',')\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Train, Validation, and Test Sets\n",
    "\n",
    "Use train to train the model, validation to compare models, and test to evaluate the final selected model.\n",
    "\n",
    "Note: using `random_state` to ensure reproducibility of splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling our Data\n",
    "\n",
    "We will use SKLearn's StandardScaler to scale our data. This will make our data have a mean of 0 and a standard deviation of 1. \n",
    "It is important to scale our validation and test sets using the scaler that was fit to the training data. Since scaling is part of our preprocessing, we need to be consistent with the scaling across all of our data (train, validation, test, and future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use standard scaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Linear Regression Model\n",
    "\n",
    "In our data science class, we've learned that the objective for linear regression is formulated as:\n",
    "\n",
    "$$\\min_{w} L(w) =  \\frac{1}{n} \\sum_{i=1}^{n} (\\phi(x_i)^Tw - y_i)^2$$\n",
    "\n",
    "where $\\phi(x_i)$ represents the feature vector of the $i^{th}$ observation, $w$ is the weight vector, and $y_i$ is the target value for the $i^{th}$ observation. We also covered that the derivative of the loss objective function \\(L(w)\\) with respect to the weight vector \\(w\\) is:\n",
    "\n",
    "$$\\frac{\\partial L(w)}{\\partial w} =  \\frac{2}{n}\\sum_{i=1}^{n} \\phi(x_i)(\\phi(x_i)^Tw - y_i)$$\n",
    "\n",
    "However, for efficiency and readability, this notebook adopts a vectorized implementation for the derivative of the loss objective function. The vectorized derivative of the loss objective function with respect to \\(w\\) is given by:\n",
    "\n",
    "$$\\frac{\\partial L(w)}{\\partial w} = \\frac{2}{n} \\Phi^T(\\Phi w - y)$$\n",
    "\n",
    "where $\\Phi$ denotes the feature matrix (with each row as a feature vector corresponding to an observation) and \\(y\\) is the vector of target values.\n",
    "\n",
    "$$\\Phi = \\begin{bmatrix} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_n)^T \\end{bmatrix}, \\quad y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n",
    "\n",
    "Below is a derivation of the vectorized implementation of the objective function derivative. You can skip the derivation if you are not interested in the details.\n",
    "\n",
    "### Derivation of the Vectorized Objective Function Derivative\n",
    "\n",
    "The objective derivative we want to derive is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} L(w) = \\frac{2}{n} \\Phi^T(\\Phi w - y)$$\n",
    "\n",
    "Substitute $\\Phi = \\begin{bmatrix} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_n)^T \\end{bmatrix}$ and $y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$ into the objective derivative:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} \\frac{1}{n} (\\Phi w - y)^2 = \\frac{2}{n} \\begin{bmatrix} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_n)^T \\end{bmatrix}^T \\left( \\begin{bmatrix} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_n)^T \\end{bmatrix} w - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\right)$$\n",
    "\n",
    "Expand the matrix multiplication:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} \\frac{1}{n} (\\Phi w - y)^2 = \\frac{2}{n} \\begin{bmatrix} \\phi(x_1) & \\phi(x_2) & \\cdots & \\phi(x_n) \\end{bmatrix} \\left( \\begin{bmatrix} \\phi(x_1)^T \\\\ \\phi(x_2)^T \\\\ \\vdots \\\\ \\phi(x_n)^T \\end{bmatrix} w - \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\right)$$\n",
    "\n",
    "combine $\\Phi w - y$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} \\frac{1}{n} (\\Phi w - y)^2 = \\frac{2}{n} \\begin{bmatrix} \\phi(x_1) & \\phi(x_2) & \\cdots & \\phi(x_n) \\end{bmatrix} \\left( \\begin{bmatrix} \\phi(x_1)^T w - y_1 \\\\ \\phi(x_2)^T w - y_2 \\\\ \\vdots \\\\ \\phi(x_n)^T w - y_n \\end{bmatrix} \\right)$$\n",
    "\n",
    "Distribute the matrix multiplication:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} L(w) = \\frac{2}{n} \\left[ \\phi(x_1) (\\phi(x_1)^T w - y_1) + \\phi(x_2) (\\phi(x_2)^T w - y_2) + \\cdots + \\phi(x_n) (\\phi(x_n)^T w - y_n) \\right] $$\n",
    "\n",
    "Simply the equation:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w} L(w)= \\frac{2}{n} \\sum_{i=1}^{n} \\phi(x_i) (\\phi(x_i)^T w - y_i)$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{2}{n} \\Phi^T(\\Phi w - y) \\equiv \\frac{2}{n}  \\sum_{i=1}^{n} \\phi(x_i) (\\phi(x_i)^T w - y_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay that was a lot of math, let's get to the code!\n",
    "\n",
    "Lets implement $\\frac{\\partial}{\\partial w} L(w) = \\frac{2}{n} \\Phi^T(\\Phi w - y)$ and use it to build our gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ߜL(w: ndarray, Φ: ndarray, y: ndarray) -> ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the loss function L with respect to weights (w).\n",
    "\n",
    "    Parameters:\n",
    "    - w: ndarray, The weight vector.\n",
    "    - Φ: ndarray, The feature matrix after applying the basis function. (Phi)\n",
    "    - y: ndarray, The target values.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: Gradient of the loss function L with respect to w.\n",
    "    \"\"\"\n",
    "    n, m = Φ.shape\n",
    "    return 1/n * Φ.T.dot(Φ.dot(w) - y)\n",
    "\n",
    "def gradient_descent(Φ: ndarray, y: ndarray, α: float = 0.01, num_iter: int = 1000) -> ndarray:\n",
    "    \"\"\"\n",
    "    Performs gradient descent to optimize w.\n",
    "\n",
    "    Parameters:\n",
    "    - Φ: ndarray, The feature matrix (Phi).\n",
    "    - y: ndarray, The target values.\n",
    "    - α: float, The learning rate.\n",
    "    - num_iter: int, The number of training iterations.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: The optimized weights vector.\n",
    "    \"\"\"\n",
    "    n, m = Φ.shape\n",
    "    w = np.zeros((m, 1))\n",
    "    for _ in range(num_iter):\n",
    "        gradient = ߜL(w, Φ, y)  # Gradient of L with respect to w\n",
    "        \n",
    "        # Checking for convergence (See note below)\n",
    "        if np.all(np.abs(gradient) < 1e-5) or np.isnan(gradient).any():\n",
    "            break\n",
    "            \n",
    "        # Check for gradient explosion (See note below)\n",
    "        if np.isinf(gradient).any(): \n",
    "            raise ValueError(\"Gradient exploded\")\n",
    "\n",
    "        w -= α * gradient\n",
    "    return w\n",
    "\n",
    "def predict(Φ: ndarray, w: ndarray) -> ndarray:\n",
    "    \"\"\"\n",
    "    Predicts the target values using the linear model.\n",
    "\n",
    "    Parameters:\n",
    "    - Φ: ndarray, The feature matrix. (Phi)\n",
    "    - w: ndarray, The weights vector.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: Predicted values.\n",
    "    \"\"\"\n",
    "    return Φ.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Gradient Descent Implementation\n",
    "\n",
    "I added a few lines that may look foreign to some of you. Specifically, the \"checking for convergence\" and \"gradient explosion\" sections. These are important to ensure that our gradient descent algorithm is working properly. \n",
    "\n",
    "Some of you may have experienced your weight vector filling with values equal to `np.inf`. This is a result of your gradients walking you further and further away from the minimum at each step. If this happens, you will need to adjust your learning rate. \n",
    "\n",
    "Conversly, if your weight vector seems to include `np.nan` values, this is a result of your gradients being too small. This either may mean you are at the minimum and you can stop training, or your learning rate is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Closed Form Solution For Linear Regression\n",
    "\n",
    "As we learned in class, since the linear regression objective function is convex, we can solve for the optimal weight vector using the closed form solution:\n",
    "\n",
    "$$w = (\\Phi^T\\Phi)^{-1}\\Phi^Ty$$\n",
    "\n",
    "Like above, $\\Phi$ denotes the feature matrix and \\(y\\) is the vector of target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_solution(Φ: ndarray, y: ndarray) -> ndarray:\n",
    "    \"\"\"\n",
    "    Computes the closed form solution for linear regression weights.\n",
    "\n",
    "    Parameters:\n",
    "    - Φ: ndarray, The feature matrix after applying the basis function. (Phi)\n",
    "    - y: ndarray, The target values.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: The weight vector that minimizes the loss function.\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(Φ.T.dot(Φ)).dot(Φ.T).dot(y)\n",
    "\n",
    "def mse(y: ndarray, y_hat: ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error (MSE) between actual and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y: ndarray, Actual target values.\n",
    "    - y_hat: ndarray, Predicted values by the model.\n",
    "\n",
    "    Returns:\n",
    "    - float: The mean squared error between actual and predicted values.\n",
    "    \"\"\"\n",
    "    return np.mean((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Linear Regression Model\n",
    "\n",
    "Before we can train or predict with our model, we need to implement to make sure our data is in the correct format. Specifically, we need to feature map our data to add a bias term!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_feature_map(X):\n",
    "    return np.hstack((np.ones((X.shape[0], 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_linear_train = linear_feature_map(X_train)\n",
    "X_linear_val = linear_feature_map(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Linear GD Train MSE: 0.041752878953118994\n",
      "My Linear GD Validation MSE: 0.05992399950038505\n"
     ]
    }
   ],
   "source": [
    "linear_gd_w = gradient_descent(X_linear_train, y_train, num_iter=10_000, α=0.01)\n",
    "pred_train = predict(X_linear_train, linear_gd_w)\n",
    "print(f\"My Linear GD Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = predict(X_linear_val, linear_gd_w)\n",
    "print(f\"My Linear GD Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving with Closed Form Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Linear Closed Form Train MSE: 0.041752878453990054\n",
      "My Linear Closed Form Validation MSE: 0.05992272985435261\n"
     ]
    }
   ],
   "source": [
    "linear_cfs_w = closed_form_solution(X_linear_train, y_train)\n",
    "pred_train = predict(X_linear_train, linear_cfs_w)\n",
    "print(f\"My Linear Closed Form Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = predict(X_linear_val, linear_cfs_w)\n",
    "print(f\"My Linear Closed Form Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn Linear Regression\n",
    "\n",
    "No need to linear feature map X as SKlearn does it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearn Linear Train MSE: 0.04175287845399005\n",
      "SKLearn Linear Validation MSE: 0.05992272985435297\n"
     ]
    }
   ],
   "source": [
    "sk_lr = SKLinearRegression() \n",
    "sk_lr.fit(X_train,y_train.flatten()) # y is 2D, but scikit-learn expects 1D\n",
    "pred_train = sk_lr.predict(X_train).reshape(-1,1)\n",
    "print(f\"SKLearn Linear Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = sk_lr.predict(X_val).reshape(-1,1)\n",
    "print(f\"SKLearn Linear Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression time!\n",
    "\n",
    "Instead of implementing a polynomial feature map, like we did for our linear model, we can use SKLearn's `PolynomialFeatures` to do it for us. This will save us a lot of time and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_train)\n",
    "X_poly_val = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Polynomial Regression Model (Gradient Descent)\n",
    "\n",
    "Now that we have feature mapped our data to include polynomial features, we can train our polynomial regression model using gradient descent. Because of the feature mapping, we don't need to make any changes to our gradient descent algorithm, we can use it as is. This is the beauty of feature mapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Polynomial GD Train MSE: 0.03166616796874941\n",
      "My Polynomial GD Validation MSE: 0.09333443610453591\n"
     ]
    }
   ],
   "source": [
    "poly_gd_w = gradient_descent(X_poly_train, y_train, num_iter=10_000, α=0.01)\n",
    "pred_train = predict(X_poly_train, poly_gd_w)\n",
    "print(f\"My Polynomial GD Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = predict(X_poly_val, poly_gd_w)\n",
    "print(f\"My Polynomial GD Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Polynomial Regression Model (Closed Form Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Polynomial Closed Form Train MSE: 0.03166484523384303\n",
      "My Polynomial Closed Form Validation MSE: 0.09294325090399753\n"
     ]
    }
   ],
   "source": [
    "poly_cfs_w = closed_form_solution(X_poly_train, y_train)\n",
    "pred_train = predict(X_poly_train, poly_cfs_w)\n",
    "print(f\"My Polynomial Closed Form Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = predict(X_poly_val, poly_cfs_w)\n",
    "print(f\"My Polynomial Closed Form Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearn Polynomial Train MSE: 0.031664845233843046\n",
      "SKLearn Polynomial Validation MSE: 0.09294325090399327\n"
     ]
    }
   ],
   "source": [
    "sk_poly_lr = SKLinearRegression()\n",
    "sk_poly_lr.fit(X_poly_train,y_train.flatten()) # y is 2D, but scikit-learn expects 1D\n",
    "pred_train = sk_poly_lr.predict(X_poly_train).reshape(-1,1) # SKLinearRegression.predict() returns 1D array, but we want 2D for my mse function\n",
    "print(f\"SKLearn Polynomial Train MSE: {mse(y_train, pred_train)}\")\n",
    "pred_val = sk_poly_lr.predict(X_poly_val).reshape(-1,1)\n",
    "print(f\"SKLearn Polynomial Validation MSE: {mse(y_val, pred_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Closed Form MSE on Validation Set: 0.05992272985435261\n",
      "Polynomial Closed Form MSE on Validation Set: 0.09294325090399753\n"
     ]
    }
   ],
   "source": [
    "print(f'Linear Closed Form MSE on Validation Set: {mse(y_val, predict(X_linear_val, linear_cfs_w))}')\n",
    "print(f'Polynomial Closed Form MSE on Validation Set: {mse(y_val, predict(X_poly_val, poly_cfs_w))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What model should we use? Polynomial or Linear?\n",
    "\n",
    "While the polynomial seems to fit better to the training data, it doesn't seem to generalize well to the validation data (low train mse, high validation mse). The linear model seems to generalize better to the validation data (higher train mse, low validation mse). Moreover, the linear model seems to be better here.\n",
    "\n",
    "We can end our experiment here by testing the better model on the test data. This score will give us an idea of how well our model will perform on future data and since we have not used the test data to train or select our model, we can be confident in the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Closed Form MSE on Test Set: 0.04931245936959388\n"
     ]
    }
   ],
   "source": [
    "print(f'Linear Closed Form MSE on Test Set: {mse(y_test, predict(linear_feature_map(X_test), linear_cfs_w))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
