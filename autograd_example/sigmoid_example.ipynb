{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T14:26:25.757446Z",
     "iopub.status.busy": "2024-09-15T14:26:25.756913Z",
     "iopub.status.idle": "2024-09-15T14:26:25.762649Z",
     "shell.execute_reply": "2024-09-15T14:26:25.761674Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Automatically find the gradient of a function\n",
    "# Download the package at : https://github.com/HIPS/autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T14:26:25.766691Z",
     "iopub.status.busy": "2024-09-15T14:26:25.766426Z",
     "iopub.status.idle": "2024-09-15T14:26:25.919885Z",
     "shell.execute_reply": "2024-09-15T14:26:25.919077Z"
    }
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.numpy import exp as e\n",
    "from autograd.numpy import power\n",
    "from autograd.numpy import transpose as T\n",
    "from autograd import grad\n",
    "#import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T14:26:25.924269Z",
     "iopub.status.busy": "2024-09-15T14:26:25.924059Z",
     "iopub.status.idle": "2024-09-15T14:26:25.928015Z",
     "shell.execute_reply": "2024-09-15T14:26:25.927311Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[1],[2]])\n",
    "W = np.array([\t[0.8,0.3],\n",
    "\t\t\t\t[0.1,0.3],\n",
    "\t\t\t\t[0.3,0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T14:26:25.932341Z",
     "iopub.status.busy": "2024-09-15T14:26:25.932112Z",
     "iopub.status.idle": "2024-09-15T14:26:25.937040Z",
     "shell.execute_reply": "2024-09-15T14:26:25.936067Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(w): \n",
    "\tv = e(-np.dot(T(w),x))\n",
    "\treturn 1/(1+v)\n",
    "#\n",
    "def ߜf(w): \n",
    "\tv = e(-np.dot(T(w),x))\n",
    "\treturn (v/((1+v)*(1+v)))*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T14:26:25.941267Z",
     "iopub.status.busy": "2024-09-15T14:26:25.941008Z",
     "iopub.status.idle": "2024-09-15T14:26:25.954342Z",
     "shell.execute_reply": "2024-09-15T14:26:25.953662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8]\n",
      " [0.3]]\n",
      "Autogen Gradient :  [0.1586849  0.31736979]\n",
      "Theoretical :  [0.1586849  0.31736979]\n",
      "\n",
      "\n",
      "\n",
      "[[0.1]\n",
      " [0.3]]\n",
      "Autogen Gradient :  [0.22171287 0.44342575]\n",
      "Theoretical :  [0.22171287 0.44342575]\n",
      "\n",
      "\n",
      "\n",
      "[[0.3]\n",
      " [0.3]]\n",
      "Autogen Gradient :  [0.20550031 0.41100061]\n",
      "Theoretical :  [0.20550031 0.41100061]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf = grad(f)       # Obtain its gradient function\n",
    "for w in W:\n",
    "\tw = np.reshape(w, (2,1))\n",
    "\tprint(w)\n",
    "\tprint('Autogen Gradient : ', pf(w).flatten())\n",
    "\tprint('Theoretical : ', ߜf(w).flatten()) \n",
    "\tprint('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
