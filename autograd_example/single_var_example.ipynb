{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# Automatically find the gradient of a function\n", "# Download the package at : https://github.com/HIPS/autograd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import autograd.numpy as np\n", "from autograd.numpy import log as ln\n", "from autograd.numpy import exp\n", "from autograd import grad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Given the function<br>\n", "$$f(x) = \\log_3(2x^2) - 2x e^{3x} + 2$$<br>\n", "The derivative should be<br>\n", "$$f'(x) = \\frac{2}{x \\ln{3}} - 2 e^{3x} - 6 x e^{3x} $$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def f(x): \n", "\treturn ln(2*x*x)/ln(3) - 2*x*exp(3*x) + 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def \u1401f(x):\n", "\treturn 2/(x*ln(3)) - 2*exp(3*x) - 6*x*exp(3*x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["auto_grad = grad(f)  # Automatically obtain the gradient function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range(10):\n", "\tx = np.random.randn()\n", "\tprint('Auto \u1401f : %.3f, Theoretical \u1401f %.3f'%(auto_grad(x), \u1401f(x)))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}