{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# -*- coding: utf-8 -*-"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Written by Chieh Wu, 9/13/2022<br>\n", "This function calculates the Gaussian Kernel by approximate it through Random fourier Feature and Orthogonal Random Feature technique."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Random Fourier Feature**<br>\n", "Given data $X \\in \\mathbb{R}^{n \\times d}$, RFF says that a kernel function is <br>\n", "$$k(x,y) = E_{p(\\omega)}[ cos(w^T x + \\theta) cos(w^T y + \\theta)] $$<br>\n", "and therefore can be approximated with<br>\n", "$$k(x,y) = \\frac{2}{m} \\sum_i^m cos(w_i^T x + \\theta) cos(w_i^T y + \\theta) $$<br>\n", "This implies that the feature map is <br>\n", "$$ \\phi(x) \\approx \\sqrt{\\frac{2}{m}} [cos(w_1^T x + \\theta), cos(w_2^T x + \\theta), ... , cos(w_m^T x + \\theta)]$$<br>\n", "Given $W \\in \\mathbb{R}^{d \\times m}$ the entire dataset $X$, then<br>\n", "$$ \\phi(X) \\approx \\sqrt{\\frac{2}{m}} cos[XW + \\theta]$$<br>\n", "The the element of the W matrix is a normal gaussian distribution divided by \u03c3 of the gaussian distribution<br>\n", "\u03b8 is randomly generated using uniform distribution between 0 to 2\u03c0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Structured Orthogonal Random Feature SORF**<br>\n", "The SORF replaces the $W$ matrix with<br>\n", "$$W = \\frac{\\sqrt{d}}{\\sigma} HD_1HD_2HD_3$$<br>\n", "$H$ is a normalized Hadamard matrix, the normalization is such that $I = H^TH$<br>\n", "Therefore $H$ is the Hadamard matrix divided by $\\frac{1}{\\sqrt{d}}$<br>\n", "and D is a diagonal matrix with the diagonal elements sampled from Rademacher Distribution"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that since Hadamard matrix size is alway in the power of 2 $2^n$, the dimension of the data is probably not the same.<br>\n", "In these cases, pad the data with 0s as the extra dimensions. Also, the width of the SORF is always set to the dimension<br>\n", "of the data after padding. Therefore, to make the width even longer, we must generate multiple size d matrices and <br>\n", "concatenate them together. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import sklearn.metrics\n", "import torch\n", "import numpy.matlib\n", "from torch.autograd import Variable\n", "from numpy.random import rand\n", "import torch.nn.functional as F\n", "from scipy.linalg import hadamard\n", "from tools import *\n", "from sklearn.metrics import mean_absolute_error\n", "from sklearn.metrics import mean_squared_error"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class random_feature:\n", "\t# sample_num, the larger the better approximation\n", "\t# random_feature_method can be 'orthogonal' (default) or 'rff'\n", "\tdef __init__(self, kernel='rbk', sigma=1, random_feature_method='orthogonal', sample_num=482):\n", "\t\tself.method = random_feature_method\n", "\t\tself.kernel = kernel\n", "\t\tself.m = sample_num\t\t# the number of samples used to approximate k(x,y)\n", "\t\tself.\u03b8 = None\t\t\t# phase shift\n", "\t\tself.\u03c3 = sigma\n", "#\n", "\tdef sample_rademacher_distribution(self, num_of_samples):\n", "\t\tr = np.random.binomial(1, 0.5, int(num_of_samples))\n", "\t\tr = 2*(r - 0.5)\n", "\t\tD = np.diag(r)\n", "\t\treturn D\n", "#\n", "\tdef initialize_random_features(self, X):\n", "\t\tif self.\u03b8 == None:\n", "\t\t\tX0 = X\n", "\t\t\tN = X.shape[0]\n", "\t\t\td = X.shape[1]\n", "\t\t\t\u03c3\u02c9\u144a = 1/self.\u03c3\n", "\t\t\t\u01bb\u03c0 = 2*np.pi\n", "\t\t\tm = self.m\n", "\t\t\td\u144a\u141f\u14be = np.sqrt(d)\n", "\t\n", "\t\t\tif self.method == 'orthogonal':\t# perform SORF\n", "\t\t\t\t#\tHere we will pad the data to ensure data dimension is in power of 2 for Hadamard matrix\n", "\t\t\t\t\u1e3f = np.power(2, np.ceil(np.log2(d))) \t# dimension in power of 2 just greater than d\n", "\t\t\t\t\u0394 = int(\u1e3f - d)\n", "\t\t\t\tif \u0394 != 0:\n", "\t\t\t\t\tpad0 = np.zeros((N, \u0394))\n", "\t\t\t\t\tX0 = np.concatenate((X, pad0), axis=1)\n", "\t\t\t\t#\teach loop creates \u1e3f out of m, we need to Figure out how many times we need to regenerate \n", "\t\t\t\t#\tNote the sample_num may not be equal to the actual random feature width M\n", "\t\t\t\t#\t\tm = the number of random features width suggested by the user\n", "\t\t\t\t#\t\t\u1e3f = the number of random features generated by each hadamard matrix \n", "\t\t\t\t#\t\tM = the acutal number of random features we actually use since power of 2 is required by hadamard\n", "\t\t\t\trepeat = int(np.ceil(m/\u1e3f))\n", "\t\t\t\tW = np.empty((int(\u1e3f),0))\n", "\t\t\t\tM = int(repeat*\u1e3f)\n", "\t\t\t\tfor \u0277 in range(repeat):\n", "\t\t\t\t\t#\tGenerate H and diagonal matrix D sampled from rademacher distribution\n", "\t\t\t\t\tH = 1/np.sqrt(\u1e3f)*hadamard(\u1e3f)\t\t# normalized H to be orthonormal\n", "\t\t\t\t\tD\u144a = self.sample_rademacher_distribution(\u1e3f)\n", "\t\t\t\t\tD\u14be = self.sample_rademacher_distribution(\u1e3f)\n", "\t\t\t\t\tD\u1dbe = self.sample_rademacher_distribution(\u1e3f)\n", "\t\t\t\t\t#\n", "\t\t\t\t\tHD\u144aHD\u14beHD\u1dbe = H.dot(D\u144a).dot(H).dot(D\u14be).dot(H).dot(D\u1dbe)\n", "\t\t\t\t\t\u0174 = (d\u144a\u141f\u14be*\u03c3\u02c9\u144a*HD\u144aHD\u14beHD\u1dbe).T\t\t\t# we did follow the paper of Wx and use XW format\n", "\t\t\t\t\tW = np.concatenate((W, \u0174), axis=1)\n", "\t\t\t\t\t#\n", "\t\t\t\t\u03b8 = \u01bb\u03c0*rand(1, M)\n", "\t\t\t\treturn [W, X0, M, \u03b8]\n", "\t\t\t\t#\n", "\t\t\telif self.method == 'rff':\n", "\t\t\t\t\u03b8 = \u01bb\u03c0*rand(1, m)\n", "\t\t\t\tW = (\u03c3\u02c9\u144a)*np.random.randn(d, m)\t# random projection matrix W\n", "\t\t\t\treturn [W, X, m, \u03b8]\n", "#\n", "\tdef get_feature_map(self, X):\n", "\t\t[W,\u1e8a, m, \u03b8] = self.initialize_random_features(X)\n", "\t\t#\n", "\t\t\u1e8aW = \u1e8a.dot(W)\n", "\t\tc = np.sqrt(2.0/m)\t\n", "\t\treturn c*np.cos(\u1e8aW + \u03b8)\t\n", "#\n", "\tdef get_kernel(self, X):\n", "\t\t\u03a6 = self.get_feature_map(X)\n", "\t\tK = \u03a6.dot(\u03a6.T)\n", "\t\tK = np.maximum(0,K)\t# run a relu on the kernel so no negative values\n", "\t\tif self.kernel == 'rbk': K = np.minimum(1,K) # make sure the kernel values doesn't go beyond 1 for gaussian\n", "\t\treturn K"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "\tnp.set_printoptions(precision=3)\n", "\tnp.set_printoptions(linewidth=300)\n", "\tnp.set_printoptions(suppress=True)\n", "#\n", "\t#\tOn lower dimension data, it seems that RFF is good enough\n", "\tX = np.random.randn(7,5)\n", "\t\u03c3 = np.median(sklearn.metrics.pairwise.pairwise_distances(X))\n", "\t\u03b3 = 1.0/(2*\u03c3*\u03c3)\n", "#\n", "\tK = sklearn.metrics.pairwise.rbf_kernel(X, gamma=\u03b3)\n", "#\n", "\tsorf = random_feature(sigma=\u03c3, random_feature_method='orthogonal')\n", "\tK\u2092 = sorf.get_kernel(X)\t\t\t# kernel matrix from orthogonal\n", "#\n", "\trff = random_feature(sigma=\u03c3, random_feature_method='rff')\n", "\tK\u1d63 = rff.get_kernel(X)\t\t\t# kernel matrix from orthogonal\n", "#\n", "#\n", "\tprint_two_matrices_side_by_side(K, K\u2092, title1='Real', title2='Approx by SORF', auto_print=True)\n", "\tprint_two_matrices_side_by_side(K, K\u1d63, title1='Real', title2='Approx by RFF', auto_print=True)\n", "#\n", "\tjupyter_print('Notice that RFFperforms better on lower dimension datasets')\n", "\t\u03b5 = mean_absolute_error(K, K\u2092)\n", "\tjupyter_print('Mean absolute Error with SORF %.3f'%\u03b5)\n", "\t\u03b5 = mean_absolute_error(K, K\u1d63)\n", "\tjupyter_print('Mean absolute Error with RFF %.3f\\n'%\u03b5)\n", "#\n", "\t\u03b5 = mean_squared_error(K, K\u2092)\n", "\tjupyter_print('Mean Squared Error with SORF %.3f'%\u03b5)\n", "\t\u03b5 = mean_squared_error(K, K\u1d63)\n", "\tjupyter_print('Mean Squared Error with SORF %.3f'%\u03b5)\n", "#\n", "#\n", "\t#\tSORF starts working better when the dimension gets larger, here we use 16 dimension\n", "\tX = csv_load('../dataset/letters.csv', shuffle_samples=True)\n", "\tX = X[0:600,:]\n", "#\n", "\t\u03c3 = np.median(sklearn.metrics.pairwise.pairwise_distances(X))\n", "\t\u03b3 = 1.0/(2*\u03c3*\u03c3)\n", "#\n", "\tK = sklearn.metrics.pairwise.rbf_kernel(X, gamma=\u03b3)\n", "#\n", "\tsorf = random_feature(sigma=\u03c3, random_feature_method='orthogonal')\n", "\tK\u2092 = sorf.get_kernel(X)\t\t\t# kernel matrix from orthogonal\n", "#\n", "\trff = random_feature(sigma=\u03c3, random_feature_method='rff')\n", "\tK\u1d63 = rff.get_kernel(X)\t\t\t# kernel matrix from orthogonal\n", "#\n", "\tjupyter_print('Notice that SORF performs better on higher dimension datasets')\n", "\t\u03b5 = mean_absolute_error(K, K\u2092)\n", "\tjupyter_print('\\tMean absolute Error with SORF %.3f'%\u03b5)\n", "\t\u03b5 = mean_absolute_error(K, K\u1d63)\n", "\tjupyter_print('\\tMean absolute Error with RFF %.3f\\n'%\u03b5)\n", "#\n", "\t\u03b5 = mean_squared_error(K, K\u2092)\n", "\tjupyter_print('\\tMean Squared Error with SORF %.3f'%\u03b5)\n", "\t\u03b5 = mean_squared_error(K, K\u1d63)\n", "\tjupyter_print('\\tMean Squared Error with SORF %.3f'%\u03b5)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}