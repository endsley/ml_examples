{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T13:21:22.426317Z",
     "iopub.status.busy": "2024-06-05T13:21:22.425941Z",
     "iopub.status.idle": "2024-06-05T13:21:22.430135Z",
     "shell.execute_reply": "2024-06-05T13:21:22.429479Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T13:21:22.436391Z",
     "iopub.status.busy": "2024-06-05T13:21:22.436015Z",
     "iopub.status.idle": "2024-06-05T13:21:23.169142Z",
     "shell.execute_reply": "2024-06-05T13:21:23.168397Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "from numpy import mean\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation for Categorical Distribution<br>\n",
    "In this example, we are going to estimate the parameters of a Categorical Distribution with 3 categories<br>\n",
    "\twhere the data $\\mathcal{D}$ consists of 1 red, 2 blue, and 4 green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic counting and recognize distribution<br>\n",
    "Using this approach, we simply need to recognize that this situation has 3 outcomes.<br>\n",
    "The Categorical Distribution is commonly used as the default structure of the pdf.<br>\n",
    "In general, if we approximate p(x) with Categorical, then we assume a structure of <br>\n",
    "$$p(x) = \\theta_1^{x_1} \\theta_2^{x_2} \\theta_3^{x_3} \\quad \\text{where} \\quad x_1,x_2,x_3 \\in \\{0,1\\} \\; and \\; x_1 + x_2 + x_3 = 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to MLE, $\\theta_i$ is the probability of success for each category.<br>\n",
    "Given 1 red, 2 blue, and 4 green, the p(x) would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x) = (1/7)^{x_1} (2/7)^{x_2} (4/7)^{x_3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $x_1, x_2, x_3$ here denotes the category and __not the sample id__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum A Posteriori Estimation (MAP)<br>\n",
    "The MAP estimation is more complicated. Here, instead of maximizing the $p(X=\\mathcal{D}|\\theta)$, we want to find $\\max \\; p(\\theta|X=\\mathcal{D})$. <br>\n",
    "In other word, we want to find the most likely $\\theta$ giving the entire dataset $X=\\mathcal{D}$.<br>\n",
    "Take a quick second to distinguish the difference between MLE and MAP<br>\n",
    "- MLE : <br>\n",
    "$$ \\max_{\\theta} \\; p(X=\\mathcal{D}|\\theta) $$<br>\n",
    "- MAP : <br>\n",
    "$$ \\max_{\\theta} \\; p(\\theta|X=\\mathcal{D}) $$<br>\n",
    "With this method, we use the Bayes' Theorem <br>\n",
    "$$ p(\\theta | X=\\mathcal{D}) = \\frac{p(X=\\mathcal{D}|\\theta) p(\\theta)}{p(X=\\mathcal{D})} $$<br>\n",
    "From MLE, we knew tht <br>\n",
    "$$p(X=\\mathcal{D}|\\theta) = \\mathcal{L} = \\prod_{i=1}^n \\; \\theta_1^{x_{i1}} \\theta_2^{x_{i2}} \\theta_3^{x_{i3}}$$<br>\n",
    "Note that for $x_{ij}$, $i$ represents the sample id, and $j$ represents the category id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With MLE, the likelihood function is sufficient. <br>\n",
    "- With MAP, it allow us to use prior knowledge about the distribution of $\\theta$. The MAP estimate consequently combines our prior knowledge with the data and come up with the best estimation. <br>\n",
    "- In this particular example, we use a Dirichlet distribution with $\\alpha_1 = 2, \\alpha_2 = 2, \\alpha_3 = 2$. <br>\n",
    "$$ p(\\theta) = \\frac{\\Gamma(\\alpha_1 + \\alpha_2 + \\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)} \\theta_1^{\\alpha_1 - 1} \\theta_2^{\\alpha_2 - 1} \\theta_3^{\\alpha_3 - 1} \\quad \\text{where if $n$ is an integer then} \\quad \\Gamma(n) = (n-1)!$$<br>\n",
    "Note that the $\\Gamma(z)$ function is much more complicated if $z$ is not an integer, specifically, it is <br>\n",
    "$$ \\Gamma(z) = \\int_0^{\\infty} \\; t^{z - 1} e^{-t} \\; dt $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\tApplying the Conjugate Priors<br>\n",
    "To obtain the posterior, we apply the conjugate prior of categorical distribution, which is a Dirichlet distribution<br>\n",
    "$$ p(\\theta) = \\frac{\\Gamma(\\alpha_1 + \\alpha_2 + \\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)} \\theta_1^{\\alpha_1 - 1} \\theta_2^{\\alpha_2 - 1} \\theta_3^{\\alpha_3 - 1} = B(\\alpha) \\theta_1^{\\alpha_1 - 1} \\theta_2^{\\alpha_2 - 1} \\theta_3^{\\alpha_3 - 1} \\quad \\text{where if $n$ is an integer then} \\quad \\Gamma(n) = (n-1)!$$<br>\n",
    "Therefore, the posterior is <br>\n",
    "$$p(\\theta|X=\\mathcal{D}) = \\frac{p(X=\\mathcal{D}|\\theta) p(\\theta)}{p(X=\\mathcal{D})}$$<br>\n",
    "implying that<br>\n",
    "$$p(\\theta|X=\\mathcal{D}) \\propto \\left( \\prod_{i=1}^n \\; \\theta_1^{x_{i1}} \\theta_2^{x_{i2}} \\theta_3^{x_{i3}} \\right) \\left( B(\\alpha) \\theta_1^{\\alpha_1 - 1} \\theta_2^{\\alpha_2 - 1} \\theta_3^{\\alpha_3 - 1}  \\right)$$<br>\n",
    "$$p(\\theta|X=\\mathcal{D}) \\propto \\left( \\theta_1^{\\sum_i x_{i1}} \\theta_2^{\\sum_i x_{i2}} \\theta_3^{\\sum_i x_{i3}} \\right) \\left( B(\\alpha) \\theta_1^{\\alpha_1 - 1} \\theta_2^{\\alpha_2 - 1} \\theta_3^{\\alpha_3 - 1}  \\right)$$<br>\n",
    "$$p(\\theta|X=\\mathcal{D}) \\propto B(\\alpha) \\left( \\theta_1^{\\sum_i x_{i1} + \\alpha_1 - 1} \\theta_2^{\\sum_i x_{i2} + \\alpha_2 - 1} \\theta_3^{\\sum_i x_{i3} + \\alpha_3 - 1} \\right).$$<br>\n",
    "This tells us that the posterior is also a Dirichlet distribution where we let<br>\n",
    "$$ \\hat{\\alpha_1} = \\sum_i x{i1} + \\alpha_1$$<br>\n",
    "$$ \\hat{\\alpha_2} = \\sum_i x{i2} + \\alpha_2$$<br>\n",
    "$$ \\hat{\\alpha_3} = \\sum_i x{i3} + \\alpha_3$$<br>\n",
    "Then<br>\n",
    "$$p(\\theta|X=\\mathcal{D}) = \\frac{\\Gamma(\\hat{\\alpha_1} + \\hat{\\alpha_2} + \\hat{\\alpha_3} )}{\\Gamma(\\hat{\\alpha_1})\\Gamma(\\hat{\\alpha_2})\\Gamma(\\hat{\\alpha_3})} \\left( \\theta_1^{\\hat{\\alpha_1} - 1} \\theta_2^{\\hat{\\alpha_2} - 1} \\theta_3^{\\hat{\\alpha_3} - 1} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\tThe Predictive Posterior<br>\n",
    "$$p(x|X=\\mathcal{D}) = \\int \\; p(x|\\theta) p(\\theta|X = \\mathcal{D}) \\; d\\theta $$<br>\n",
    "$$p(x|X=\\mathcal{D}) = \\int \\; \\theta_1^{x_1} \\theta_2^{x_2} \\theta_3^{x_3} B(\\hat{\\alpha})  \\theta_1^{\\hat{\\alpha_1} - 1} \\theta_2^{\\hat{\\alpha_2} - 1} \\theta_3^{\\hat{\\alpha_3} - 1} \\; d\\theta$$<br>\n",
    "If we let <br>\n",
    "$$\\bar{\\alpha}_1 = x_1 + \\hat{\\alpha_1}$$<br>\n",
    "$$\\bar{\\alpha}_2 = x_2 + \\hat{\\alpha_2}$$<br>\n",
    "$$\\bar{\\alpha}_3 = x_3 + \\hat{\\alpha_3}$$<br>\n",
    "then we can rewrite the integral as <br>\n",
    "$$\\frac{B(\\hat{\\alpha})}{B(\\bar{\\alpha})} \\; \\int \\; B(\\bar{\\alpha}) \\theta_1^{\\bar{\\alpha_1} - 1} \\theta_2^{\\bar{\\alpha_2} - 1} \\theta_3^{\\bar{\\alpha_3} - 1} \\; d\\theta= \\frac{B(\\hat{\\alpha})}{B(\\bar{\\alpha})}$$<br>\n",
    "We can further simplify this into<br>\n",
    "$$\\frac{\\Gamma(\\hat{\\alpha_1} + \\hat{\\alpha_2} + \\hat{\\alpha_3})}{\\Gamma(\\hat{\\alpha_1})\\Gamma(\\hat{\\alpha_2})\\Gamma(\\hat{\\alpha_3})} \\frac{\\Gamma(\\bar{\\alpha_1})\\Gamma(\\bar{\\alpha_2})\\Gamma(\\bar{\\alpha_3})}{\\Gamma(\\bar{\\alpha_1} + \\bar{\\alpha_2} + \\bar{\\alpha_3})} $$<br>\n",
    "Remember that $x = [x_1\\quad x_2\\quad x_3]^{\\top}$ and  $x_i \\in \\{0,1\\}$ and $x_1 + x_2 + x_3 = 1$, this allows us to further simplify this into<br>\n",
    "$$p(x|X=\\mathcal{D}) = \\frac{ \\hat{\\alpha_1}^{x_1} \\hat{\\alpha_2}^{x_2} \\hat{\\alpha_3}^{x_3}}{ \\hat{\\alpha_1} + \\hat{\\alpha_2} + \\hat{\\alpha_3} }$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
